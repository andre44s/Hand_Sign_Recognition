{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Hand_Sign_Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1gkEWg1MdzA"
      },
      "source": [
        "# Import Library\n",
        "!pip install split-folders\n",
        "\n",
        "import os\n",
        "import numpy\n",
        "import shutil\n",
        "import filecmp\n",
        "import splitfolders\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications import ResNet152V2\n",
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGMHqZsBTse2"
      },
      "source": [
        "# Google drive mounting\n",
        "drive.mount('/content/gdrive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
        "%cd /content/gdrive/My Drive/Kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vclWiLFQYwEO"
      },
      "source": [
        "# Download dataset from kaggle\n",
        "!kaggle datasets download -d muhammadkhalid/sign-language-for-alphabets\n",
        "!kaggle datasets download -d grassknoted/asl-alphabet\n",
        "!kaggle datasets download -d ahmedkhanak1995/sign-language-gesture-images-dataset\n",
        "!kaggle datasets download -d kapillondhe/american-sign-language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Ozfib_Y6Un"
      },
      "source": [
        "# Creating folder for dataset\n",
        "raw_data = '/content/gdrive/MyDrive/Kaggle/raw_data'\n",
        "home_dir = '/content/gdrive/MyDrive/Kaggle/dataset'\n",
        "final_dir = '/content/gdrive/MyDrive/Kaggle/final'\n",
        "\n",
        "if not os.path.exists(raw_data):\n",
        "    os.mkdir(raw_data)\n",
        "    print('Folder raw_data created')\n",
        "else:\n",
        "    print('Folder raw_data already exist')\n",
        "\n",
        "if not os.path.exists(home_dir):\n",
        "    os.mkdir(home_dir)\n",
        "    print('Folder home_dir created')\n",
        "else:\n",
        "    print('Folder home_dir already exist')\n",
        "\n",
        "if not os.path.exists(final_dir):\n",
        "    os.mkdir(final_dir)\n",
        "    print('Folder final_dir created')\n",
        "else:\n",
        "    print('Folder final_dir already exist')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ceRNiqIeAa"
      },
      "source": [
        "# Extracting the first dataset\n",
        "zip_loc1 = '/content/gdrive/MyDrive/Kaggle/sign-language-for-alphabets.zip'\n",
        "with ZipFile(zip_loc1, 'r') as zip1:\n",
        "    zip1.extractall(path=raw_data)\n",
        "print('Done extracting first dataset')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vzB0N3QIery"
      },
      "source": [
        "# Extracting the second dataset\n",
        "zip_loc2 = '/content/gdrive/MyDrive/Kaggle/asl-alphabet.zip'\n",
        "with ZipFile(zip_loc2, 'r') as zip2:\n",
        "    zip2.extractall(path=raw_data)\n",
        "print('Done extracting second dataset')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Xlcc9vxJ12"
      },
      "source": [
        "# Extracting the third dataset\n",
        "zip_loc3 = '/content/gdrive/MyDrive/Kaggle/sign-language-gesture-images-dataset.zip'\n",
        "with ZipFile(zip_loc3, 'r') as zip3:\n",
        "    zip3.extractall(path=raw_data)\n",
        "print('Done extracting third dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DviPGMExKcV"
      },
      "source": [
        "# Extracting the fourth dataset\n",
        "zip_loc4 = '/content/gdrive/MyDrive/Kaggle/american-sign-language.zip'\n",
        "with ZipFile(zip_loc4, 'r') as zip4:\n",
        "    zip4.extractall(path=raw_data)\n",
        "print('Done extracting fourth dataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUIryqyJDsp1"
      },
      "source": [
        "# Deleting unecesarry file from the dataset\n",
        "del_list = ['/content/gdrive/MyDrive/Kaggle/raw_data/asl_alphabet_train/asl_alphabet_train/del',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/asl_alphabet_train/asl_alphabet_train/nothing',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/asl_alphabet_train/asl_alphabet_train/space',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/Sign Language for Alphabets/unknown',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/asl_alphabet_test',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/Gesture Image Pre-Processed Data',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/ASL_Dataset/Test',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/Gesture Image Data/_',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/ASL_Dataset/Train/Nothing',\n",
        "            '/content/gdrive/MyDrive/Kaggle/raw_data/ASL_Dataset/Train/Space'\n",
        "            ]\n",
        "\n",
        "for o in range(0,10):\n",
        "    del_list.append(os.path.join('/content/gdrive/MyDrive/Kaggle/raw_data/Gesture Image Data', str(o)))\n",
        "\n",
        "for i in del_list:\n",
        "    if not os.path.exists(i):\n",
        "        print(f'Folder {i} not found')\n",
        "    else:\n",
        "        shutil.rmtree(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1D0z6y0ismv"
      },
      "source": [
        "# Preparing for data merging\n",
        "dataset1_dir = '/content/gdrive/MyDrive/Kaggle/raw_data/Sign Language for Alphabets'\n",
        "dataset2_dir = '/content/gdrive/MyDrive/Kaggle/raw_data/asl_alphabet_train/asl_alphabet_train'\n",
        "dataset3_dir = '/content/gdrive/MyDrive/Kaggle/raw_data/Gesture Image Data'\n",
        "dataset4_dir = '/content/gdrive/MyDrive/Kaggle/raw_data/ASL_Dataset/Train'\n",
        "class_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
        "              'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "\n",
        "for i in class_list:\n",
        "    current_wd = os.path.join(home_dir, i)\n",
        "    if not os.path.exists(current_wd):\n",
        "        os.mkdir(current_wd)\n",
        "    else:\n",
        "        print(f'Folder {i} already exist')\n",
        "\n",
        "print('Done')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbt7_P9Q99VW"
      },
      "source": [
        "# Copy the first dataset\n",
        "x = -2\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset1_dir):\n",
        "    print('Copy ==> ' + dirpath)\n",
        "    x = x + 1\n",
        "    for i in filenames:\n",
        "        src = os.path.join(dirpath, i)\n",
        "        dst = os.path.join(home_dir, class_list[x], 'AA' + i)\n",
        "        if not os.path.exists(dst) or not filecmp.cmp(src, dst):\n",
        "            shutil.copyfile(src, dst)\n",
        "        else:\n",
        "            print(f'File {i} already exist')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AUtaJYr1Px0"
      },
      "source": [
        "# Copy the second dataset\n",
        "x = -2\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset2_dir):\n",
        "    print('Copy ==> ' + dirpath)\n",
        "    x = x + 1\n",
        "    for i in filenames:\n",
        "        src = os.path.join(dirpath, i)\n",
        "        dst = os.path.join(home_dir, class_list[x], 'BB' + i)\n",
        "        if not os.path.exists(dst) or not filecmp.cmp(src, dst):\n",
        "            shutil.copyfile(src, dst)\n",
        "        else:\n",
        "            print(f'File {i} already exist')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxuJG5UQ6DUc"
      },
      "source": [
        "# Copy the third dataset\n",
        "x = -2\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset3_dir):\n",
        "    print('Copy ==> ' + dirpath)\n",
        "    x = x + 1\n",
        "    for i in filenames:\n",
        "        src = os.path.join(dirpath, i)\n",
        "        dst = os.path.join(home_dir, class_list[x], 'CC' + i)\n",
        "        if not os.path.exists(dst) or not filecmp.cmp(src, dst):\n",
        "            shutil.copyfile(src, dst)\n",
        "        else:\n",
        "            print(f'File {i} already exist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAGexwFA6FiL"
      },
      "source": [
        "# Copy the fourth dataset\n",
        "x = -2\n",
        "for (dirpath, dirnames, filenames) in os.walk(dataset4_dir):\n",
        "    print('Copy ==> ' + dirpath)\n",
        "    x = x + 1\n",
        "    for i in filenames:\n",
        "        src = os.path.join(dirpath, i)\n",
        "        dst = os.path.join(home_dir, class_list[x], 'DD' + i)\n",
        "        if not os.path.exists(dst) or not filecmp.cmp(src, dst):\n",
        "            shutil.copyfile(src, dst)\n",
        "        else:\n",
        "            print(f'File {i} already exist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9nxu0STxjq8"
      },
      "source": [
        "# Splitting the dataset to train and validation folder\n",
        "splitfolders.ratio(home_dir, \n",
        "                   output=final_dir, \n",
        "                   seed=796234751384, \n",
        "                   ratio=(0.8, 0.2), \n",
        "                   group_prefix=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHwjKOYfkT_b"
      },
      "source": [
        "# Deleting unecesarry file from the folder\n",
        "shutil.rmtree('/content/gdrive/MyDrive/Kaggle/final/train/.ipynb_checkpoints')\n",
        "shutil.rmtree('/content/gdrive/MyDrive/Kaggle/final/val/.ipynb_checkpoints')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZJe2SCmnRpU"
      },
      "source": [
        "# Import ImageDataGenerator for image augmentation\n",
        "train_dir = '/content/gdrive/MyDrive/Kaggle/final/train'\n",
        "valid_dir = '/content/gdrive/MyDrive/Kaggle/final/val'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    brightness_range=(0.5, 1.0),\n",
        "    shear_range=0.2,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "valid_datagen = ImageDataGenerator(\n",
        "    rescale=1./255)\n",
        "\n",
        "# Generator for training\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    batch_size=256,\n",
        "    class_mode='categorical',\n",
        ")\n",
        "\n",
        "# Generator for validation\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=(224, 224),\n",
        "    color_mode='rgb',\n",
        "    batch_size=256,\n",
        "    class_mode='categorical',\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxdXzIVZ1hFT"
      },
      "source": [
        "# Callback for validation accuracy \n",
        "class myCallbacks(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_accuracy') >= .92 and logs.get('accuracy') >= .92):\n",
        "            self.model.stop_training = True\n",
        "            print('Accuracy Complete')\n",
        "\n",
        "callbacks = myCallbacks()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6zMWD7i2U5p"
      },
      "source": [
        "# Model creation\n",
        "# Model uses transfer learning from ResNet152V2\n",
        "model = tf.keras.Sequential([\n",
        "    ResNet152V2(weights=\"imagenet\",\n",
        "                include_top=False,\n",
        "                input_tensor=Input(shape=(224, 224, 3))),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(26, activation='softmax')\n",
        "])\n",
        "\n",
        "model.layers[0].trainable = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nergTxVu4L3R"
      },
      "source": [
        "# Compiling the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sM0pb364Xaq"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=valid_generator,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    callbacks=[callbacks]\n",
        "                    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__UHoDH67uv"
      },
      "source": [
        "# Plotting the loss and accuracy of train and validation\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\n",
        "fig.suptitle('Loss and Accuracy')\n",
        "\n",
        "ax1.plot(history.history['accuracy'])\n",
        "ax1.plot(history.history['val_accuracy'])\n",
        "ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "ax2.plot(history.history['loss'])\n",
        "ax2.plot(history.history['val_loss'])\n",
        "ax2.legend(['Train', 'Validation'])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-5199kNOqmN"
      },
      "source": [
        "# Convert the model to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbz89CPIwxXV"
      },
      "source": [
        "# Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# Serialize weights to HDF5\n",
        "model.save_weights(\"weights.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}